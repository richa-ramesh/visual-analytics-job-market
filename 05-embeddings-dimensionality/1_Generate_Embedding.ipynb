{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759d1a80",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f6fa0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['job_title', 'seniority_level', 'status', 'company', 'location', 'post_date', 'headquarter', 'industry', 'ownership', 'company_size', 'revenue', 'salary', 'skills', 'state', 'fips', 'fips_int', 'salary_mid', 'seniority_level_norm', 'skills_list', 'skills_clean']\n",
      "\n",
      "Value counts:\n",
      "seniority_level\n",
      "senior      629\n",
      "lead        116\n",
      "midlevel    112\n",
      "NaN          60\n",
      "junior       24\n",
      "Name: count, dtype: int64\n",
      "['hybrid' 'on-site' 'remote']\n",
      "['Retail' 'Manufacturing' 'Technology' 'Finance' 'Education' 'Healthcare'\n",
      " 'Energy' 'Logistics']\n",
      "0                               Grapevine, TX . Hybrid\n",
      "1                              Fort Worth, TX . Hybrid\n",
      "2    Austin, TX . Toronto, Ontario, Canada . Kirkla...\n",
      "3    Chicago, IL . Scottsdale, AZ . Austin, TX . Hy...\n",
      "4                                              On-site\n",
      "5                                         New York, NY\n",
      "6                                         Berkeley, CA\n",
      "7                                       Menlo Park, CA\n",
      "8                                         Fully Remote\n",
      "9                                              On-site\n",
      "Name: location, dtype: object\n",
      "0    ['spark', 'r', 'python', 'scala', 'machine lea...\n",
      "1    ['spark', 'r', 'python', 'sql', 'machine learn...\n",
      "2    ['aws', 'git', 'python', 'docker', 'sql', 'mac...\n",
      "3                               ['sql', 'r', 'python']\n",
      "4                                                   []\n",
      "5    ['scikit-learn', 'python', 'scala', 'sql', 'ma...\n",
      "6                                                   []\n",
      "7                                 ['machine learning']\n",
      "8                                                   []\n",
      "9                                                   []\n",
      "Name: skills, dtype: object\n",
      "0    €100,472 - €200,938\n",
      "1               €118,733\n",
      "2     €94,987 - €159,559\n",
      "3    €112,797 - €194,402\n",
      "4    €114,172 - €228,337\n",
      "5    €196,371 - €251,170\n",
      "6      €51,330 - €70,144\n",
      "7    €121,480 - €132,440\n",
      "8               €207,331\n",
      "9               €219,201\n",
      "Name: salary, dtype: object\n",
      "0    €352.44B\n",
      "1     155,030\n",
      "2      25,930\n",
      "3      34,690\n",
      "4       1,800\n",
      "5         150\n",
      "6      17,471\n",
      "7         900\n",
      "8         126\n",
      "9       5,520\n",
      "Name: company_size, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import List, Optional, Dict\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"../data/data_science_job_posts_2025_clean.csv\")\n",
    "\n",
    "# Show all column names\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# helps with mapping\n",
    "print(\"\\nValue counts:\")\n",
    "print(df[\"seniority_level\"].value_counts(dropna=False))\n",
    "print(df[\"status\"].dropna().unique())\n",
    "print(df[\"industry\"].dropna().unique())\n",
    "print(df[\"location\"].head(10))\n",
    "print(df[\"skills\"].head(10))\n",
    "print(df[\"salary\"].head(10))\n",
    "print(df[\"company_size\"].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd098eae",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06e4dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from: ../data/data_science_job_posts_2025_clean.csv\n",
      "Building skill vocabulary from skills_clean...\n",
      "Top skills: ['python', 'machine learning', 'sql', 'r', 'aws', 'deep learning', 'tensorflow', 'spark', 'azure', 'pytorch', 'tableau', 'gcp', 'scikit-learn', 'scala', 'database', 'pandas', 'java', 'hadoop', 'git', 'numpy', 'docker', 'amazon', 'kubernetes', 'matplotlib', 'keras', 'powerbi', 'airflow', 'linux', 'neural network', 'scipy', 'bash', 'sklearn', 'opencv']\n",
      "Using salary_mid from cleaned schema...\n",
      "Encoding seniority_level_norm...\n",
      "Encoding status (hybrid, on-site, remote)...\n",
      "Encoding industry...\n",
      "Encoding ownership...\n",
      "Encoding state from cleaned column...\n",
      "Bucketing company_size...\n",
      "Parsing post_date as days-ago...\n",
      "Scaling fips_int as a simple region feature...\n",
      "Encoding job_title families...\n",
      "Final embedding shape: (941, 106)\n",
      "Writing full embeddings to: ../data/data_science_job_posts_2025_embeddings_full.csv\n",
      "Running PCA to project embeddings to 2D...\n",
      "2D embeddings shape: (941, 3)\n",
      "Writing 2D embeddings to: ../data/data_science_job_posts_2025_embeddings_2d.csv\n",
      "Writing merged cleaned data + 2D embeddings to: ../data/data_science_job_posts_2025_with_embeddings_2d.csv\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "INPUT_PATH = \"../data/data_science_job_posts_2025_clean.csv\"\n",
    "\n",
    "# High-dimensional numeric embedding (all engineered features)\n",
    "OUTPUT_EMBEDDINGS_FULL = \"../data/data_science_job_posts_2025_embeddings_full.csv\"\n",
    "\n",
    "# 2D projection for visualization (id, x, y)\n",
    "OUTPUT_EMBEDDINGS_2D = \"../data/data_science_job_posts_2025_embeddings_2d.csv\"\n",
    "\n",
    "# merged cleaned data + x,y\n",
    "OUTPUT_MERGED_WITH_2D = \"../data/data_science_job_posts_2025_with_embeddings_2d.csv\"\n",
    "\n",
    "# lookup tables for encoding categories\n",
    "SENIORITY_MAP: Dict[str, int] = {\n",
    "    \"junior\": 1,\n",
    "    \"midlevel\": 2,\n",
    "    \"senior\": 3,\n",
    "    \"lead\": 4,\n",
    "}\n",
    "\n",
    "STATUS_VALUES = [\"hybrid\", \"on-site\", \"remote\"]\n",
    "\n",
    "INDUSTRY_VALUES = [\n",
    "    \"Retail\",\n",
    "    \"Manufacturing\",\n",
    "    \"Technology\",\n",
    "    \"Finance\",\n",
    "    \"Education\",\n",
    "    \"Healthcare\",\n",
    "    \"Energy\",\n",
    "    \"Logistics\",\n",
    "]\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "# Parse skills_clean into a list of lowercase skill tokens.\n",
    "def parse_skills_clean(skills_str: str | float) -> List[str]:\n",
    "    if pd.isna(skills_str):\n",
    "        return []\n",
    "    parts = str(skills_str).split(\"|\")\n",
    "    return [p.strip().lower() for p in parts if p.strip()]\n",
    "\n",
    "\n",
    "# Build a vocabulary of top-N most frequent skills from the cleaned skills_clean column.\n",
    "def build_skill_vocab(df: pd.DataFrame, column: str = \"skills_clean\", top_n: int = 50) -> List[str]:\n",
    "    from collections import Counter\n",
    "\n",
    "    counter = Counter()\n",
    "    for s in df[column]:\n",
    "        counter.update(parse_skills_clean(s))\n",
    "\n",
    "    vocab = [skill for skill, _ in counter.most_common(top_n)]\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# Parse '17 days ago', '1 day ago', '30+ days ago' -> numeric days.\n",
    "def parse_post_age_days(s: str | float) -> Optional[float]:\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    text = str(s).lower()\n",
    "    m = re.search(r\"(\\d+)\", text)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(1))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Map normalized seniority labels to small integers.\n",
    "def map_seniority_level(s: str | float) -> Optional[int]:\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    text = str(s).strip().lower()\n",
    "    return SENIORITY_MAP.get(text)\n",
    "\n",
    "\n",
    "# Min-max scale a numeric series to [0, 1].\n",
    "def min_max_scale(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(float)\n",
    "    mn = s.min()\n",
    "    mx = s.max()\n",
    "    if pd.isna(mn) or pd.isna(mx) or mn == mx:\n",
    "        return pd.Series(0.0, index=s.index)\n",
    "    return (s - mn) / (mx - mn)\n",
    "\n",
    "\n",
    "#  < 1,000  -> 'small'\n",
    "#  < 10,000 -> 'medium'\n",
    "#  >=10,000 -> 'large'\n",
    "def bucket_company_size(size_str: str | float) -> str:\n",
    "    if pd.isna(size_str):\n",
    "        return \"unknown\"\n",
    "\n",
    "    text = str(size_str)\n",
    "    if \"€\" in text or \"b\" in text.lower() or \"m\" in text.lower():\n",
    "        return \"very_large\"\n",
    "\n",
    "    m = re.search(r\"[\\d,]+(?:\\.\\d+)?\", text)\n",
    "    if not m:\n",
    "        return \"unknown\"\n",
    "\n",
    "    try:\n",
    "        n = float(m.group(0).replace(\",\", \"\"))\n",
    "    except ValueError:\n",
    "        return \"unknown\"\n",
    "\n",
    "    if n < 1000:\n",
    "        return \"small\"\n",
    "    elif n < 10000:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main embedding builder\n",
    "# -----------------------------\n",
    "# Take the cleaned jobs DataFrame and build a numeric embedding matrix.\n",
    "# Each row corresponds to one job; columns are simple numeric features.\n",
    "def build_embeddings(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # Add a simple integer ID if not present (for joining later)\n",
    "    if \"id\" not in df.columns:\n",
    "        df[\"id\"] = np.arange(len(df))\n",
    "\n",
    "    # 1) Skills: build multi-hot columns from skills_clean\n",
    "    print(\"Building skill vocabulary from skills_clean...\")\n",
    "    skill_vocab = build_skill_vocab(df, column=\"skills_clean\", top_n=50)\n",
    "    print(f\"Top skills: {skill_vocab}\")\n",
    "    df[\"_skills_list\"] = df[\"skills_clean\"].apply(parse_skills_clean)\n",
    "\n",
    "    for sk in skill_vocab:\n",
    "        col = f\"skill_{sk.replace(' ', '_')}\"\n",
    "        df[col] = df[\"_skills_list\"].apply(lambda lst, sk=sk: int(sk in lst))\n",
    "\n",
    "    # 2) Salary: use the cleaned salary_mid column\n",
    "    print(\"Using salary_mid from cleaned schema...\")\n",
    "    df[\"salary_mid\"] = pd.to_numeric(df[\"salary_mid\"], errors=\"coerce\")\n",
    "    df[\"salary_mid_scaled\"] = min_max_scale(df[\"salary_mid\"])\n",
    "\n",
    "    # 3) Seniority: use seniority_level_norm -> numeric level\n",
    "    print(\"Encoding seniority_level_norm...\")\n",
    "    df[\"seniority_level_num\"] = df[\"seniority_level_norm\"].apply(map_seniority_level)\n",
    "    median_level = df[\"seniority_level_num\"].median()\n",
    "    df[\"seniority_level_num\"] = df[\"seniority_level_num\"].fillna(median_level)\n",
    "\n",
    "    # 4) Status (hybrid / on-site / remote): simple one-hot encoding\n",
    "    print(\"Encoding status (hybrid, on-site, remote)...\")\n",
    "    df[\"status_clean\"] = df[\"status\"].astype(str).str.strip().str.lower()\n",
    "    for val in STATUS_VALUES:\n",
    "        col = f\"status_{val.replace('-', '_')}\"\n",
    "        df[col] = (df[\"status_clean\"] == val).astype(int)\n",
    "\n",
    "    # 5) Industry: one-hot only for a small fixed set of industries\n",
    "    print(\"Encoding industry...\")\n",
    "    df[\"industry_clean\"] = df[\"industry\"].astype(str).str.strip()\n",
    "    for val in INDUSTRY_VALUES:\n",
    "        col = f\"industry_{val.lower()}\"\n",
    "        df[col] = (df[\"industry_clean\"] == val).astype(int)\n",
    "\n",
    "    # 6) Ownership: one-hot for all observed values\n",
    "    print(\"Encoding ownership...\")\n",
    "    df[\"ownership_clean\"] = df[\"ownership\"].astype(str).str.strip()\n",
    "    ownership_dummies = pd.get_dummies(df[\"ownership_clean\"], prefix=\"ownership\", dtype=int)\n",
    "    df = pd.concat([df, ownership_dummies], axis=1)\n",
    "\n",
    "    # 7) State: use the cleaned 'state' column directly\n",
    "    print(\"Encoding state from cleaned column...\")\n",
    "    df[\"state_clean\"] = df[\"state\"].astype(str).str.strip().str.upper()\n",
    "    state_dummies = pd.get_dummies(df[\"state_clean\"], prefix=\"state\", dtype=int)\n",
    "    df = pd.concat([df, state_dummies], axis=1)\n",
    "\n",
    "    # 8) Company size: bucket into small / medium / large / very_large\n",
    "    print(\"Bucketing company_size...\")\n",
    "    df[\"company_size_bucket\"] = df[\"company_size\"].apply(bucket_company_size)\n",
    "    size_dummies = pd.get_dummies(df[\"company_size_bucket\"], prefix=\"company_size\", dtype=int)\n",
    "    df = pd.concat([df, size_dummies], axis=1)\n",
    "\n",
    "    # 9) Post date: convert '17 days ago' style strings -> recency score\n",
    "    print(\"Parsing post_date as days-ago...\")\n",
    "    df[\"post_age_days\"] = df[\"post_date\"].apply(parse_post_age_days)\n",
    "    df[\"post_age_days\"] = df[\"post_age_days\"].fillna(df[\"post_age_days\"].max())\n",
    "    df[\"post_age_scaled\"] = min_max_scale(df[\"post_age_days\"])\n",
    "\n",
    "    # 10) FIPS: use the integer FIPS code as a simple location feature\n",
    "    print(\"Scaling fips_int as a simple region feature...\")\n",
    "    df[\"fips_int\"] = pd.to_numeric(df[\"fips_int\"], errors=\"coerce\")\n",
    "    df[\"fips_int_scaled\"] = min_max_scale(df[\"fips_int\"])\n",
    "\n",
    "    # 11) Job title flags: simple text-based job family indicators\n",
    "    print(\"Encoding job_title families...\")\n",
    "\n",
    "    def job_flags(title: str | float) -> Dict[str, int]:\n",
    "        t = \"\" if pd.isna(title) else str(title).lower()\n",
    "        return {\n",
    "            \"job_is_data_scientist\": int(\"data scientist\" in t),\n",
    "            \"job_is_data_engineer\": int(\"data engineer\" in t),\n",
    "            \"job_is_ml_engineer\": int(\"machine learning engineer\" in t or \"ml engineer\" in t),\n",
    "            \"job_is_analyst\": int(\"analyst\" in t),\n",
    "        }\n",
    "\n",
    "    job_flags_df = pd.DataFrame(list(df[\"job_title\"].apply(job_flags)))\n",
    "    df = pd.concat([df, job_flags_df], axis=1)\n",
    "\n",
    "    # 12) Select the final embedding columns in a fixed order\n",
    "    embed_cols: List[str] = [\"id\"]\n",
    "\n",
    "    # skills\n",
    "    embed_cols += [c for c in df.columns if c.startswith(\"skill_\")]\n",
    "\n",
    "    # salary\n",
    "    embed_cols.append(\"salary_mid_scaled\")\n",
    "\n",
    "    # seniority\n",
    "    embed_cols.append(\"seniority_level_num\")\n",
    "\n",
    "    # status, industry, ownership, state, company_size\n",
    "    embed_cols += [c for c in df.columns if c.startswith(\"status_\")]\n",
    "    embed_cols += [c for c in df.columns if c.startswith(\"industry_\")]\n",
    "    embed_cols += [c for c in df.columns if c.startswith(\"ownership_\")]\n",
    "    embed_cols += [c for c in df.columns if c.startswith(\"state_\")]\n",
    "    embed_cols += [c for c in df.columns if c.startswith(\"company_size_\")]\n",
    "\n",
    "    # post age + FIPS\n",
    "    embed_cols.append(\"post_age_scaled\")\n",
    "    embed_cols.append(\"fips_int_scaled\")\n",
    "\n",
    "    # job flags\n",
    "    embed_cols += [\n",
    "        \"job_is_data_scientist\",\n",
    "        \"job_is_data_engineer\",\n",
    "        \"job_is_ml_engineer\",\n",
    "        \"job_is_analyst\",\n",
    "    ]\n",
    "\n",
    "    # Build numeric embedding DataFrame and fill any remaining NaNs with 0.0\n",
    "    embeddings = df[embed_cols].copy()\n",
    "    embeddings = embeddings.apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    print(\"Final embedding shape:\", embeddings.shape)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# 2D Projection with PCA\n",
    "def project_embeddings_to_2d(embeddings: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Take the high-dimensional embeddings DataFrame (including 'id'),\n",
    "    run PCA to project into 2D, and return a DataFrame with columns: id, x, y.\n",
    "    \"\"\"\n",
    "    ids = embeddings[\"id\"].values\n",
    "    features = embeddings.drop(columns=[\"id\"]).values\n",
    "\n",
    "    print(\"Running PCA to project embeddings to 2D...\")\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    coords = pca.fit_transform(features)\n",
    "\n",
    "    emb2d = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": ids,\n",
    "            \"x\": coords[:, 0],\n",
    "            \"y\": coords[:, 1],\n",
    "        }\n",
    "    )\n",
    "    print(\"2D embeddings shape:\", emb2d.shape)\n",
    "    return emb2d\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Reading data from: {INPUT_PATH}\")\n",
    "    raw = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "    if \"id\" not in raw.columns:\n",
    "        raw[\"id\"] = np.arange(len(raw))\n",
    "\n",
    "    # High-dimensional embeddings\n",
    "    embeddings_full = build_embeddings(raw)\n",
    "\n",
    "    print(f\"Writing full embeddings to: {OUTPUT_EMBEDDINGS_FULL}\")\n",
    "    embeddings_full.to_csv(OUTPUT_EMBEDDINGS_FULL, index=False)\n",
    "\n",
    "    # 2D projection\n",
    "    embeddings_2d = project_embeddings_to_2d(embeddings_full)\n",
    "\n",
    "    print(f\"Writing 2D embeddings to: {OUTPUT_EMBEDDINGS_2D}\")\n",
    "    embeddings_2d.to_csv(OUTPUT_EMBEDDINGS_2D, index=False)\n",
    "\n",
    "    merged_with_2d = raw.merge(embeddings_2d, on=\"id\", how=\"left\")\n",
    "    print(f\"Writing merged cleaned data + 2D embeddings to: {OUTPUT_MERGED_WITH_2D}\")\n",
    "    merged_with_2d.to_csv(OUTPUT_MERGED_WITH_2D, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
