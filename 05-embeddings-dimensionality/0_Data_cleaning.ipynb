{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd358a5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c482c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "from typing import Dict\n",
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd19054",
   "metadata": {},
   "source": [
    "## Dataset Cleaning \n",
    "\n",
    "This module provides a structured data-cleaning pipeline for the\n",
    "*Data Science Job Postings 2025* dataset.\n",
    "\n",
    "**Functionality:**\n",
    "\n",
    "- **State Extraction:** Uses regular expressions to extract U.S. state abbreviations from the `headquarter` column and maps them to FIPS codes (`state`, `fips`, `fips_int` columns).\n",
    "- **Salary Parsing:** Converts salary text ranges (e.g., \"$80K–$120K\") into numeric midpoints via normalization and numeric extraction (`salary_mid`).\n",
    "- **Seniority Normalization:** Cleans and standardizes the `seniority_level` column to lowercase values, replacing invalid entries.\n",
    "- **Skills Parsing:** Transforms stringified skill lists into clean Python lists (`skills_list`) by stripping brackets, quotes, and whitespace.\n",
    "- **Deduplication:** Removes duplicate job postings using key columns (`job_title`, `company`, `location`, `post_date`, `salary`, `skills`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b0fe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed cleaning. new cleaned Dataset created at → ../data/data_science_job_posts_2025_clean.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Constants ---\n",
    "STATE_ABBR = r\"\\b(AL|AK|AZ|AR|CA|CO|CT|DE|FL|GA|HI|ID|IL|IN|IA|KS|KY|LA|ME|MD|MA|MI|MN|MS|MO|MT|NE|NV|NH|NJ|NM|NY|NC|ND|OH|OK|OR|PA|RI|SC|SD|TN|TX|UT|VT|VA|WA|WV|WI|WY|DC)\\b\"\n",
    "_ABBR_TO_FIPS = {\n",
    "    'AL':'01','AK':'02','AZ':'04','AR':'05','CA':'06','CO':'08','CT':'09','DE':'10','DC':'11','FL':'12','GA':'13',\n",
    "    'HI':'15','ID':'16','IL':'17','IN':'18','IA':'19','KS':'20','KY':'21','LA':'22','ME':'23','MD':'24','MA':'25',\n",
    "    'MI':'26','MN':'27','MS':'28','MO':'29','MT':'30','NE':'31','NV':'32','NH':'33','NJ':'34','NM':'35','NY':'36',\n",
    "    'NC':'37','ND':'38','OH':'39','OK':'40','OR':'41','PA':'42','RI':'44','SC':'45','SD':'46','TN':'47','TX':'48',\n",
    "    'UT':'49','VT':'50','VA':'51','WA':'53','WV':'54','WI':'55','WY':'56'\n",
    "}\n",
    "\n",
    "_state_pat = re.compile(STATE_ABBR)\n",
    "\n",
    "\n",
    "def extract_state(headquarter: pd.Series) -> pd.Series:\n",
    "    def _one(x):\n",
    "        s = str(x).upper()\n",
    "        m = _state_pat.search(s)\n",
    "        return m.group(1) if m else None\n",
    "    return headquarter.astype(str).map(_one)\n",
    "\n",
    "\n",
    "def parse_salary(s):\n",
    "    \"\"\"Return a numeric midpoint if salary is a range or single number, else None.\"\"\"\n",
    "    s = str(s)\n",
    "    if not s or s.lower() in {\"nan\", \"none\"}:\n",
    "        return None\n",
    "    s_norm = s.replace(\"–\", \"-\").lower()\n",
    "    nums = [float(x.replace(\",\", \"\")) for x in re.findall(r\"\\d[\\d,]*\\.?\\d*\", s_norm)]\n",
    "    if not nums:\n",
    "        return None\n",
    "    mid = (nums[0] + nums[1]) / 2 if len(nums) >= 2 else nums[0]\n",
    "    if \"k\" in s_norm:\n",
    "        mid *= 1000.0\n",
    "    return mid\n",
    "\n",
    "\n",
    "def parse_skills(s):\n",
    "    \"\"\"\n",
    "    Turn \"['python', 'sql']\" or '[\"python\",\"sql\"]' or \"python, sql\" into a list.\n",
    "    Keep it simple and lowercase.\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return []\n",
    "    parts = [x.strip(\" '\\\"\").lower() for x in s.strip(\"[]\").split(\",\") if x.strip(\" '\\\"\")]\n",
    "    return parts\n",
    "\n",
    "\n",
    "def clean_jobs_df(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # --- state + fips ---\n",
    "    df[\"state\"] = extract_state(df.get(\"headquarter\"))\n",
    "    df[\"fips\"] = df[\"state\"].map(_ABBR_TO_FIPS)\n",
    "    df[\"fips_int\"] = pd.to_numeric(df[\"fips\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # --- salary ---\n",
    "    df[\"salary_mid\"] = df.get(\"salary\", pd.Series([None] * len(df))).apply(parse_salary)\n",
    "\n",
    "    # --- seniority ---\n",
    "    df[\"seniority_level_norm\"] = (\n",
    "        df.get(\"seniority_level\", pd.Series([None] * len(df)))\n",
    "          .astype(str).str.lower().str.strip()\n",
    "          .replace({\"nan\": pd.NA, \"\": pd.NA})\n",
    "    )\n",
    "\n",
    "    # --- skills ---\n",
    "    skills_parsed = df.get(\"skills\", pd.Series([None] * len(df))).apply(parse_skills)\n",
    "    df[\"skills_list\"] = skills_parsed\n",
    "    # CSV-friendly version for Altair notebook\n",
    "    df[\"skills_clean\"] = skills_parsed.apply(lambda lst: \"|\".join(lst))\n",
    "\n",
    "    # --- de-dup ---\n",
    "    dedup_keys = [\"job_title\", \"company\", \"location\", \"post_date\", \"salary\", \"skills\"]\n",
    "    existing = [c for c in dedup_keys if c in df.columns]\n",
    "    if existing:\n",
    "        df = df.drop_duplicates(subset=existing)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    in_path = \"../data/data_science_job_posts_2025.csv\"\n",
    "    out_path = \"../data/data_science_job_posts_2025_clean.csv\"\n",
    "    raw = pd.read_csv(in_path, low_memory=False)\n",
    "    clean = clean_jobs_df(raw)\n",
    "    clean.to_csv(out_path, index=False)\n",
    "    print(f\"Completed cleaning. new cleaned Dataset created at → {out_path}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
