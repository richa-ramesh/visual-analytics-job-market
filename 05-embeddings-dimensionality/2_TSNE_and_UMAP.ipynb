{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cdfadd8-328b-4a4a-8820-58352a51781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab7a17a7-7485-4050-9e48-1d1f8e0d68b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 123849 job postings\n",
      "Columns: ['job_id', 'company_name', 'title', 'description', 'max_salary', 'pay_period', 'location', 'company_id', 'views', 'med_salary', 'min_salary', 'formatted_work_type', 'applies', 'original_listed_time', 'remote_allowed', 'job_posting_url', 'application_url', 'application_type', 'expiry', 'closed_time', 'formatted_experience_level', 'skills_desc', 'listed_time', 'posting_domain', 'sponsored', 'work_type', 'currency', 'compensation_type', 'normalized_salary', 'zip_code', 'fips']\n",
      "\n",
      "Cleaned data: 22785 job postings\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('postings.csv')\n",
    "\n",
    "print(f\"Loaded {len(df)} job postings\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Basic cleaning\n",
    "data = df.copy()\n",
    "\n",
    "# Clean salary\n",
    "if 'normalized_salary' in data.columns:\n",
    "    data['salary'] = data['normalized_salary']\n",
    "else:\n",
    "    data['salary'] = data[['min_salary', 'max_salary', 'med_salary']].mean(axis=1)\n",
    "\n",
    "# Remove records with missing critical data\n",
    "data = data.dropna(subset=['salary', 'formatted_experience_level'])\n",
    "data = data[(data['salary'] >= 20000) & (data['salary'] <= 500000)]\n",
    "\n",
    "# Clean categorical fields\n",
    "data['experience_level'] = data['formatted_experience_level'].fillna('Not Specified')\n",
    "data['work_type'] = data['formatted_work_type'].fillna('Not Specified')\n",
    "data['remote'] = data['remote_allowed'].fillna(0).astype(int)\n",
    "\n",
    "# Calculate engagement rate\n",
    "data['engagement_rate'] = (data['applies'] / (data['views'] + 1)) * 100\n",
    "data['engagement_rate'] = data['engagement_rate'].clip(0, 100)\n",
    "\n",
    "# Extract state\n",
    "data['state'] = data['location'].str.split(',').str[-1].str.strip().str.upper()\n",
    "invalid_states = ['UNITED STATES', 'US', 'USA', 'REMOTE', '']\n",
    "data = data[~data['state'].isin(invalid_states)]\n",
    "data = data[data['state'].str.len() == 2]\n",
    "\n",
    "print(f\"\\nCleaned data: {len(data)} job postings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c4278e6-7f0c-4dd5-8cfa-5b5b74ffa042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING EMBEDDINGS\n",
      "================================================================================\n",
      "\n",
      "Numeric features: 4 dimensions\n",
      "Experience level features: 6 dimensions\n",
      "Work type features: 7 dimensions\n",
      "Remote feature: 1 dimension\n",
      "Region features: 5 dimensions\n",
      "Title keyword features: 12 dimensions\n",
      "\n",
      "Total embedding dimensions: 35\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING EMBEDDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# We'll create embeddings based on multiple dimensions:\n",
    "# 1. Numeric features (salary, engagement, views, applies)\n",
    "# 2. Categorical features (experience level, work type, state)\n",
    "# 3. Temporal features (if available)\n",
    "# 4. Text features (from job title - simple keyword counts)\n",
    "\n",
    "embeddings_data = data[['job_id']].copy()\n",
    "\n",
    "# --- NUMERIC FEATURES (4 dimensions) ---\n",
    "# Log-transform salary for better scale\n",
    "embeddings_data['salary_log'] = np.log1p(data['salary'])\n",
    "\n",
    "# Engagement rate (already 0-100)\n",
    "embeddings_data['engagement_rate'] = data['engagement_rate']\n",
    "\n",
    "# Log-transform views and applies\n",
    "embeddings_data['views_log'] = np.log1p(data['views'])\n",
    "embeddings_data['applies_log'] = np.log1p(data['applies'])\n",
    "\n",
    "print(f\"\\nNumeric features: 4 dimensions\")\n",
    "\n",
    "# --- CATEGORICAL FEATURES: Experience Level (one-hot encoding) ---\n",
    "exp_dummies = pd.get_dummies(data['experience_level'], prefix='exp')\n",
    "embeddings_data = pd.concat([embeddings_data, exp_dummies], axis=1)\n",
    "print(f\"Experience level features: {len(exp_dummies.columns)} dimensions\")\n",
    "\n",
    "# --- CATEGORICAL FEATURES: Work Type (one-hot encoding) ---\n",
    "work_dummies = pd.get_dummies(data['work_type'], prefix='work')\n",
    "embeddings_data = pd.concat([embeddings_data, work_dummies], axis=1)\n",
    "print(f\"Work type features: {len(work_dummies.columns)} dimensions\")\n",
    "\n",
    "# --- BINARY FEATURE: Remote ---\n",
    "embeddings_data['remote'] = data['remote']\n",
    "print(f\"Remote feature: 1 dimension\")\n",
    "\n",
    "# --- CATEGORICAL FEATURES: State (grouped by region to reduce dimensionality) ---\n",
    "# Group states into regions\n",
    "region_map = {\n",
    "    # Northeast\n",
    "    'ME': 'Northeast', 'NH': 'Northeast', 'VT': 'Northeast', 'MA': 'Northeast',\n",
    "    'RI': 'Northeast', 'CT': 'Northeast', 'NY': 'Northeast', 'NJ': 'Northeast',\n",
    "    'PA': 'Northeast', 'DE': 'Northeast', 'MD': 'Northeast', 'DC': 'Northeast',\n",
    "    # Southeast\n",
    "    'WV': 'Southeast', 'VA': 'Southeast', 'KY': 'Southeast', 'TN': 'Southeast',\n",
    "    'NC': 'Southeast', 'SC': 'Southeast', 'GA': 'Southeast', 'FL': 'Southeast',\n",
    "    'AL': 'Southeast', 'MS': 'Southeast', 'LA': 'Southeast', 'AR': 'Southeast',\n",
    "    # Midwest\n",
    "    'OH': 'Midwest', 'MI': 'Midwest', 'IN': 'Midwest', 'IL': 'Midwest',\n",
    "    'WI': 'Midwest', 'MN': 'Midwest', 'IA': 'Midwest', 'MO': 'Midwest',\n",
    "    'ND': 'Midwest', 'SD': 'Midwest', 'NE': 'Midwest', 'KS': 'Midwest',\n",
    "    # Southwest\n",
    "    'TX': 'Southwest', 'OK': 'Southwest', 'NM': 'Southwest', 'AZ': 'Southwest',\n",
    "    # West\n",
    "    'CO': 'West', 'WY': 'West', 'MT': 'West', 'ID': 'West', 'UT': 'West',\n",
    "    'NV': 'West', 'CA': 'West', 'OR': 'West', 'WA': 'West', 'AK': 'West', 'HI': 'West'\n",
    "}\n",
    "\n",
    "data['region'] = data['state'].map(region_map)\n",
    "region_dummies = pd.get_dummies(data['region'], prefix='region')\n",
    "embeddings_data = pd.concat([embeddings_data, region_dummies], axis=1)\n",
    "print(f\"Region features: {len(region_dummies.columns)} dimensions\")\n",
    "\n",
    "# --- TEXT FEATURES: Job Title Keywords (simple approach) ---\n",
    "# Extract common keywords from job titles\n",
    "keywords = ['senior', 'junior', 'lead', 'manager', 'director', 'engineer', \n",
    "            'developer', 'analyst', 'scientist', 'designer', 'data', 'software']\n",
    "\n",
    "for keyword in keywords:\n",
    "    embeddings_data[f'title_{keyword}'] = data['title'].str.lower().str.contains(keyword, na=False).astype(int)\n",
    "\n",
    "print(f\"Title keyword features: {len(keywords)} dimensions\")\n",
    "\n",
    "# Total embedding dimensions\n",
    "total_dims = len(embeddings_data.columns) - 1  # Exclude job_id\n",
    "print(f\"\\nTotal embedding dimensions: {total_dims}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b9ec305-0791-4fcc-9f13-f43d14b16c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HANDLING MISSING VALUES AND NORMALIZING\n",
      "================================================================================\n",
      "Missing values per column:\n",
      "engagement_rate    18182\n",
      "views_log            266\n",
      "applies_log        18182\n",
      "dtype: int64\n",
      "\n",
      "Features shape after filling NaN: (22785, 35)\n",
      "Scaled features shape: (22785, 35)\n",
      "\n",
      "Saved embeddings.csv (22785 records)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HANDLING MISSING VALUES AND NORMALIZING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Separate job_id from features\n",
    "job_ids = embeddings_data['job_id'].values\n",
    "feature_columns = [col for col in embeddings_data.columns if col != 'job_id']\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"Missing values per column:\")\n",
    "missing_counts = embeddings_data[feature_columns].isna().sum()\n",
    "print(missing_counts[missing_counts > 0])\n",
    "\n",
    "# Fill missing values with 0 (appropriate for one-hot encoded and log-transformed features)\n",
    "features = embeddings_data[feature_columns].fillna(0).values\n",
    "\n",
    "print(f\"\\nFeatures shape after filling NaN: {features.shape}\")\n",
    "\n",
    "# Standardize features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "print(f\"Scaled features shape: {features_scaled.shape}\")\n",
    "\n",
    "# Save full embeddings\n",
    "embeddings_df = pd.DataFrame(features_scaled, columns=feature_columns)\n",
    "embeddings_df.insert(0, 'job_id', job_ids)\n",
    "\n",
    "# Add original attributes for reference\n",
    "embeddings_df['salary'] = data['salary'].values\n",
    "embeddings_df['experience_level'] = data['experience_level'].values\n",
    "embeddings_df['work_type'] = data['work_type'].values\n",
    "embeddings_df['state'] = data['state'].values\n",
    "embeddings_df['region'] = data['region'].values\n",
    "embeddings_df['engagement_rate_orig'] = data['engagement_rate'].values\n",
    "embeddings_df['title'] = data['title'].values\n",
    "\n",
    "embeddings_df.to_csv('embeddings.csv', index=False)\n",
    "print(f\"\\nSaved embeddings.csv ({len(embeddings_df)} records)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50d61df9-75ff-44ad-822c-37b60aaa8e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DIMENSIONALITY REDUCTION: PCA\n",
      "================================================================================\n",
      "PCA explained variance ratio: [0.0900452  0.06780134]\n",
      "Total variance explained: 15.78%\n",
      "\n",
      "Saved embeddings_pca_2d.csv (22785 records)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIMENSIONALITY REDUCTION: PCA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_coords = pca.fit_transform(features_scaled)\n",
    "\n",
    "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Create PCA dataframe\n",
    "pca_df = pd.DataFrame({\n",
    "    'job_id': job_ids,\n",
    "    'pca_x': pca_coords[:, 0],\n",
    "    'pca_y': pca_coords[:, 1],\n",
    "    'salary': data['salary'].values,\n",
    "    'experience_level': data['experience_level'].values,\n",
    "    'work_type': data['work_type'].values,\n",
    "    'state': data['state'].values,\n",
    "    'region': data['region'].values,\n",
    "    'engagement_rate': data['engagement_rate'].values,\n",
    "    'remote': data['remote'].values,\n",
    "    'title': data['title'].values\n",
    "})\n",
    "\n",
    "pca_df.to_csv('embeddings_pca_2d.csv', index=False)\n",
    "print(f\"\\nSaved embeddings_pca_2d.csv ({len(pca_df)} records)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c3d7a21-aaf3-4649-ba9b-cf414790a13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DIMENSIONALITY REDUCTION: t-SNE\n",
      "================================================================================\n",
      "Using 10000 samples for t-SNE\n",
      "\n",
      "Saved embeddings_tsne_2d.csv (10000 records)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIMENSIONALITY REDUCTION: t-SNE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Apply t-SNE (on a sample if data is large)\n",
    "sample_size = min(10000, len(features_scaled))\n",
    "if len(features_scaled) > sample_size:\n",
    "    indices = np.random.choice(len(features_scaled), sample_size, replace=False)\n",
    "    features_sample = features_scaled[indices]\n",
    "    sample_job_ids = job_ids[indices]\n",
    "else:\n",
    "    features_sample = features_scaled\n",
    "    indices = np.arange(len(features_scaled))\n",
    "    sample_job_ids = job_ids\n",
    "\n",
    "print(f\"Using {len(features_sample)} samples for t-SNE\")\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "tsne_coords = tsne.fit_transform(features_sample)\n",
    "\n",
    "# Create t-SNE dataframe\n",
    "tsne_df = pd.DataFrame({\n",
    "    'job_id': sample_job_ids,\n",
    "    'tsne_x': tsne_coords[:, 0],\n",
    "    'tsne_y': tsne_coords[:, 1],\n",
    "    'salary': data.iloc[indices]['salary'].values,\n",
    "    'experience_level': data.iloc[indices]['experience_level'].values,\n",
    "    'work_type': data.iloc[indices]['work_type'].values,\n",
    "    'state': data.iloc[indices]['state'].values,\n",
    "    'region': data.iloc[indices]['region'].values,\n",
    "    'engagement_rate': data.iloc[indices]['engagement_rate'].values,\n",
    "    'remote': data.iloc[indices]['remote'].values,\n",
    "    'title': data.iloc[indices]['title'].values\n",
    "})\n",
    "\n",
    "tsne_df.to_csv('embeddings_tsne_2d.csv', index=False)\n",
    "print(f\"\\nSaved embeddings_tsne_2d.csv ({len(tsne_df)} records)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51d4233e-987d-4b52-9a2e-5d13b74e9f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DIMENSIONALITY REDUCTION: UMAP\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richa/Desktop/cs424/myenv/lib/python3.13/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved embeddings_umap_2d.csv (10000 records)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIMENSIONALITY REDUCTION: UMAP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Apply UMAP (on same sample as t-SNE for consistency)\n",
    "reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "umap_coords = reducer.fit_transform(features_sample)\n",
    "\n",
    "# Create UMAP dataframe\n",
    "umap_df = pd.DataFrame({\n",
    "    'job_id': sample_job_ids,\n",
    "    'umap_x': umap_coords[:, 0],\n",
    "    'umap_y': umap_coords[:, 1],\n",
    "    'salary': data.iloc[indices]['salary'].values,\n",
    "    'experience_level': data.iloc[indices]['experience_level'].values,\n",
    "    'work_type': data.iloc[indices]['work_type'].values,\n",
    "    'state': data.iloc[indices]['state'].values,\n",
    "    'region': data.iloc[indices]['region'].values,\n",
    "    'engagement_rate': data.iloc[indices]['engagement_rate'].values,\n",
    "    'remote': data.iloc[indices]['remote'].values,\n",
    "    'title': data.iloc[indices]['title'].values\n",
    "})\n",
    "\n",
    "umap_df.to_csv('embeddings_umap_2d.csv', index=False)\n",
    "print(f\"\\nSaved embeddings_umap_2d.csv ({len(umap_df)} records)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4044d0ad-46e4-426c-9b3a-669b4ff267c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EMBEDDING CREATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Generated files:\n",
      "1. embeddings.csv - Full high-dimensional embeddings\n",
      "2. embeddings_pca_2d.csv - PCA 2D projection\n",
      "3. embeddings_tsne_2d.csv - t-SNE 2D projection\n",
      "4. embeddings_umap_2d.csv - UMAP 2D projection\n",
      "\n",
      "Feature breakdown:\n",
      "  - Numeric features: 4 (salary_log, engagement, views_log, applies_log)\n",
      "  - Experience level: 6 dimensions\n",
      "  - Work type: 7 dimensions\n",
      "  - Remote: 1 dimension\n",
      "  - Region: 5 dimensions\n",
      "  - Title keywords: 12 dimensions\n",
      "  - TOTAL: 35 dimensions\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EMBEDDING CREATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"1. embeddings.csv - Full high-dimensional embeddings\")\n",
    "print(\"2. embeddings_pca_2d.csv - PCA 2D projection\")\n",
    "print(\"3. embeddings_tsne_2d.csv - t-SNE 2D projection\")\n",
    "print(\"4. embeddings_umap_2d.csv - UMAP 2D projection\")\n",
    "print(\"\\nFeature breakdown:\")\n",
    "print(f\"  - Numeric features: 4 (salary_log, engagement, views_log, applies_log)\")\n",
    "print(f\"  - Experience level: {len(exp_dummies.columns)} dimensions\")\n",
    "print(f\"  - Work type: {len(work_dummies.columns)} dimensions\")\n",
    "print(f\"  - Remote: 1 dimension\")\n",
    "print(f\"  - Region: {len(region_dummies.columns)} dimensions\")\n",
    "print(f\"  - Title keywords: {len(keywords)} dimensions\")\n",
    "print(f\"  - TOTAL: {total_dims} dimensions\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44f5ee-83e8-4535-b328-646f059c57d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
